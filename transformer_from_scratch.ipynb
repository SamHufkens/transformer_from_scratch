{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JNAB4Lp36F89"
      },
      "source": [
        "# Transformer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TYLZHTZD6IQV"
      },
      "source": [
        "This notebook wil contain code for creating the dataset from the sequences that were created in the previous notebook. After the dataset is created, the model will be build and created."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Ry4aiDN0B3C"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Layer, Dense, Dropout, LayerNormalization, Embedding, TextVectorization, Input, MultiHeadAttention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fkul0oVGz7Sa"
      },
      "outputs": [],
      "source": [
        "# Constants\n",
        "DATA_PATH = './data/movie_reviews.npy'\n",
        "VOCAB_PATH = './vocab.txt'\n",
        "MODEL_PATH = './model/transformer.keras'\n",
        "VOCAB_SIZE = 5000\n",
        "SEQ_LENGTH = 200\n",
        "BATCH_SIZE = 32\n",
        "EMBEDDING_DIM = 768\n",
        "DENSE_1_NEURONS = 3072\n",
        "HEADS = 12\n",
        "KEY_DIM = EMBEDDING_DIM // HEADS\n",
        "MAX_LEN_UNIQUE_POS_EMBED = 1000\n",
        "DROP_RATE=0.1\n",
        "EPOCHS = 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uDHbxMLg6dux"
      },
      "source": [
        "## 1. Load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PfvrYgIv7Lkr"
      },
      "outputs": [],
      "source": [
        "sequences = np.load(DATA_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2M-aZLVb6hDc"
      },
      "source": [
        "## 2. Create dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ybgAR_F90Ad7"
      },
      "outputs": [],
      "source": [
        "# Create Tensorflow dataset\n",
        "dataset = tf.data.Dataset.from_tensor_slices(sequences).batch(BATCH_SIZE).shuffle(1000)\n",
        "\n",
        "# Create vocab\n",
        "vectorize_layer = TextVectorization(\n",
        "    standardize='lower',\n",
        "    max_tokens=VOCAB_SIZE,\n",
        "    output_mode='int',\n",
        "    output_sequence_length=SEQ_LENGTH + 1\n",
        ")\n",
        "vectorize_layer.adapt(dataset)\n",
        "vocab = vectorize_layer.get_vocabulary()\n",
        "\n",
        "# Save vocab\n",
        "with open(VOCAB_PATH, \"w\", encoding=\"utf-8\") as f:\n",
        "    for word in vocab:\n",
        "        f.write(word + \"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e6Am7Qrx5IXk"
      },
      "outputs": [],
      "source": [
        "# Create the train dataset with x and y\n",
        "def create_train_dataset(text):\n",
        "    text = tf.expand_dims(text, -1)\n",
        "    tokenized_sentences = vectorize_layer(text)\n",
        "    x = tokenized_sentences[:, :-1]\n",
        "    y = tokenized_sentences[:, 1:]\n",
        "    return x, y\n",
        "train_dataset = dataset.map(create_train_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DiG4I2ZF6k_J"
      },
      "source": [
        "## 3. Transformer model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PrRhzLl1UVvl"
      },
      "outputs": [],
      "source": [
        "class PositionalEmbedding(Layer):\n",
        "  def __init__(self, vocab_size, embedding_dim, max_unique_pos_embed, **kwargs):\n",
        "    super(PositionalEmbedding, self).__init__(**kwargs)\n",
        "    self.vocab_size = vocab_size\n",
        "    self.max_unique_pos_embed = max_unique_pos_embed\n",
        "    self.embedding_dim = embedding_dim\n",
        "    self.token_embedding_layer = Embedding(input_dim=vocab_size, output_dim=embedding_dim)\n",
        "    self.position_embedding_layer = Embedding(input_dim=max_unique_pos_embed, output_dim=embedding_dim)\n",
        "\n",
        "  def call(self, x):\n",
        "\n",
        "    maxlen = shape(x)[-1]\n",
        "\n",
        "    positions = range(start=0, limit=maxlen, delta=1)\n",
        "    positions = self.position_embedding_layer(positions)\n",
        "\n",
        "    x = self.token_embedding_layer(x)\n",
        "\n",
        "    output = x + positions\n",
        "\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "DKLzfHzT6q00",
        "outputId": "eb2857f2-7ca8-49de-c7fa-3de1fee29011"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Output shape: (32, 200, 64)\n",
            "tf.Tensor(\n",
            "[[[-0.01684658 -0.02843867  0.13636388 ... -0.01269964  0.21264365\n",
            "   -0.10245864]\n",
            "  [ 0.05744156 -0.07425009 -0.16975887 ...  0.14768438 -0.1101056\n",
            "   -0.15561865]\n",
            "  [ 0.10218031  0.05113341 -0.00909905 ... -0.00203522 -0.06763754\n",
            "    0.07966353]\n",
            "  ...\n",
            "  [ 0.02877847  0.00839305 -0.11123461 ... -0.06628624  0.06627677\n",
            "   -0.01747258]\n",
            "  [-0.03113098  0.1329103   0.03487611 ... -0.03849317  0.12371826\n",
            "   -0.05110065]\n",
            "  [ 0.04300503 -0.01798058 -0.07151026 ... -0.1153248   0.02136786\n",
            "   -0.01359177]]\n",
            "\n",
            " [[-0.04039139 -0.11168819 -0.20691945 ...  0.19812664  0.12778094\n",
            "   -0.02614967]\n",
            "  [-0.01306725  0.10088292  0.01217076 ...  0.09959167  0.00678044\n",
            "    0.19562873]\n",
            "  [ 0.04125831 -0.03818874 -0.12444432 ...  0.09522966 -0.05494731\n",
            "    0.05533848]\n",
            "  ...\n",
            "  [-0.07768421 -0.00514741  0.00772872 ...  0.17633076  0.19051944\n",
            "   -0.05156904]\n",
            "  [ 0.17175764  0.3017796  -0.07001042 ...  0.08506124 -0.00249469\n",
            "    0.21139127]\n",
            "  [-0.07381722  0.02054632  0.01932745 ...  0.13741696 -0.07606091\n",
            "   -0.07116105]]\n",
            "\n",
            " [[ 0.15196218 -0.04249929 -0.04890931 ... -0.0884645   0.11734658\n",
            "    0.00164757]\n",
            "  [-0.14913364 -0.10757774 -0.09134626 ... -0.07444738  0.10921127\n",
            "   -0.04222576]\n",
            "  [-0.14382076  0.19112813 -0.1613766  ... -0.08701643  0.07491115\n",
            "   -0.1591067 ]\n",
            "  ...\n",
            "  [-0.08561562 -0.07418302  0.09006275 ...  0.07599471  0.12060061\n",
            "    0.04554312]\n",
            "  [-0.23178679 -0.20200121 -0.05747453 ... -0.06023117  0.00947974\n",
            "    0.00991136]\n",
            "  [-0.22886343 -0.10725451 -0.10234649 ... -0.01520304 -0.25732508\n",
            "   -0.1033984 ]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[-0.06033909  0.01557756  0.08149891 ...  0.08597052 -0.06561894\n",
            "   -0.10959634]\n",
            "  [-0.01554704 -0.05161596  0.01263259 ...  0.0678125  -0.0641001\n",
            "   -0.06005445]\n",
            "  [ 0.00875711 -0.03313569  0.06435071 ...  0.09456574 -0.01897655\n",
            "   -0.04327177]\n",
            "  ...\n",
            "  [ 0.1228828  -0.06204103  0.12564476 ...  0.00845405  0.07119734\n",
            "   -0.12478444]\n",
            "  [-0.16746533 -0.01795258  0.00781613 ...  0.16044176  0.20709229\n",
            "   -0.03190987]\n",
            "  [-0.17100212  0.08792696  0.08719007 ...  0.0649971   0.14160992\n",
            "   -0.07984474]]\n",
            "\n",
            " [[-0.03920501  0.03489303 -0.036483   ...  0.04151827  0.00699872\n",
            "   -0.00863438]\n",
            "  [-0.05896237  0.07753509  0.1358455  ...  0.02950758 -0.01221719\n",
            "   -0.02832314]\n",
            "  [ 0.05508302  0.12669356  0.29924664 ... -0.0282961  -0.07681968\n",
            "   -0.04938298]\n",
            "  ...\n",
            "  [ 0.17236397 -0.02356656  0.12282387 ...  0.14784735 -0.07568979\n",
            "   -0.10829476]\n",
            "  [ 0.06954537  0.06856025  0.33937857 ... -0.03680547 -0.29649705\n",
            "   -0.03041453]\n",
            "  [-0.04555625 -0.01470563  0.11020437 ... -0.04366318 -0.04232337\n",
            "   -0.13592689]]\n",
            "\n",
            " [[ 0.016037    0.01468785  0.01884243 ... -0.21261206 -0.1195691\n",
            "   -0.15311095]\n",
            "  [ 0.12230565 -0.09876996 -0.12671879 ...  0.05663895 -0.09238277\n",
            "   -0.0157527 ]\n",
            "  [ 0.26174402 -0.00576312  0.02211772 ... -0.03803171  0.0285117\n",
            "    0.00161736]\n",
            "  ...\n",
            "  [ 0.06984805  0.14378513 -0.10515002 ...  0.09702151 -0.00339049\n",
            "    0.1902894 ]\n",
            "  [-0.18169352  0.0280874   0.12924513 ...  0.25464243 -0.10285857\n",
            "    0.03830481]\n",
            "  [ 0.16975532 -0.1166182  -0.04516079 ... -0.1706892  -0.04476936\n",
            "   -0.13069825]]], shape=(32, 200, 64), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "class SelfAttention(Layer):\n",
        "  def __init__(self, **kwargs):\n",
        "    super(SelfAttention, self).__init__(**kwargs)\n",
        "\n",
        "  def call(self, query, keys, values, keys_dim, mask=None):\n",
        "\n",
        "    # Calculate scores\n",
        "    scores = tf.matmul(query, keys, transpose_b=True) / tf.math.sqrt(tf.cast(keys_dim, tf.float32))\n",
        "\n",
        "    # Prevent attention to future tokens\n",
        "    if mask is not None:\n",
        "      mask = tf.cast(mask, dtype=tf.float32)\n",
        "\n",
        "      # Expand mask shape to [batch_size, 1, seq_len, seq_len]\n",
        "      mask = tf.expand_dims(mask, axis=1)\n",
        "      scores += (mask * -1e9)\n",
        "\n",
        "    # Calculate weights with softmax\n",
        "    attention_weights = tf.nn.softmax(scores, axis=-1)\n",
        "\n",
        "    # Calculate the output\n",
        "    output = tf.matmul(attention_weights, values)\n",
        "\n",
        "    return output\n",
        "\n",
        "# Test\n",
        "query = tf.random.normal((BATCH_SIZE, SEQ_LENGTH, KEY_DIM))\n",
        "keys = tf.random.normal((BATCH_SIZE, SEQ_LENGTH, KEY_DIM))\n",
        "values = tf.random.normal((BATCH_SIZE, SEQ_LENGTH, KEY_DIM))\n",
        "\n",
        "self_attention = SelfAttention()\n",
        "output = self_attention(query, keys, values, keys_dim=KEY_DIM)\n",
        "\n",
        "print(f\"Output shape: {output.shape}\")\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "01XRYAVP2yFs",
        "outputId": "5bf11a32-9aad-4111-97ec-7403086981ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Output shape: (32, 200, 768)\n",
            "tf.Tensor(\n",
            "[[[ 2.21717469e-02  4.14373726e-02  7.46395905e-03 ...  1.71128183e-03\n",
            "   -5.63040702e-03 -2.83440622e-03]\n",
            "  [ 3.24134529e-02  3.97914462e-02  4.77555487e-03 ...  1.08526545e-02\n",
            "   -8.11995193e-03 -1.24990698e-02]\n",
            "  [ 3.19103301e-02  4.00614031e-02  1.12193609e-02 ...  8.76557548e-03\n",
            "   -5.27278567e-03 -3.98530671e-03]\n",
            "  ...\n",
            "  [ 3.28454003e-02  3.24166082e-02  9.83953848e-03 ...  1.29540237e-02\n",
            "    9.36508295e-04 -1.09021310e-02]\n",
            "  [ 3.23331542e-02  3.87845188e-02  1.29571361e-02 ...  5.07120136e-03\n",
            "   -9.18035582e-03 -7.84869771e-03]\n",
            "  [ 2.99939029e-02  2.72923280e-02  4.18264326e-03 ...  1.57208573e-02\n",
            "   -1.68871526e-02 -3.01272538e-03]]\n",
            "\n",
            " [[-5.26845502e-03 -7.46474368e-03 -1.49262454e-02 ...  1.92249212e-02\n",
            "   -2.65123993e-02 -5.84297115e-03]\n",
            "  [ 2.59523652e-03  1.70633532e-04 -6.06783386e-03 ...  1.71769988e-02\n",
            "   -4.06142361e-02 -8.70768074e-03]\n",
            "  [-1.69234851e-03 -4.12062509e-03 -5.39701944e-03 ...  1.94731969e-02\n",
            "   -3.56496237e-02 -4.51655639e-03]\n",
            "  ...\n",
            "  [ 1.46704062e-03  1.05298287e-03 -1.44436304e-03 ...  1.29643567e-02\n",
            "   -4.35592495e-02 -7.71825295e-03]\n",
            "  [ 4.65364242e-03 -6.59348962e-06 -1.26106050e-02 ...  1.88740101e-02\n",
            "   -3.51341330e-02 -2.50842981e-03]\n",
            "  [-2.92941870e-04  1.33950845e-03 -4.29355400e-03 ...  1.87601596e-02\n",
            "   -3.06998510e-02 -1.19005954e-02]]\n",
            "\n",
            " [[ 5.36923646e-04 -1.98288914e-03  4.83465940e-02 ... -2.42108740e-02\n",
            "    1.93585325e-02 -5.44322878e-02]\n",
            "  [ 4.69194958e-03 -9.65980813e-03  3.75272445e-02 ... -1.72549654e-02\n",
            "    3.38132717e-02 -7.57204145e-02]\n",
            "  [ 2.54697842e-03 -2.22940766e-03  3.33837606e-02 ... -1.18877580e-02\n",
            "    3.22112478e-02 -7.42565244e-02]\n",
            "  ...\n",
            "  [ 4.76764189e-03 -4.61678905e-03  4.78373207e-02 ... -1.99899245e-02\n",
            "    2.76015177e-02 -7.66632855e-02]\n",
            "  [-1.78493687e-03 -4.82346676e-03  4.77622263e-02 ... -1.28289126e-02\n",
            "    3.26070227e-02 -7.21755624e-02]\n",
            "  [ 2.15141312e-03 -4.03308962e-03  3.82731967e-02 ... -8.18203110e-03\n",
            "    3.25833783e-02 -7.47008324e-02]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[ 1.23379212e-02  1.01082372e-02 -3.43711488e-02 ...  3.90052376e-03\n",
            "   -3.33150998e-02  3.93756619e-03]\n",
            "  [ 1.75322909e-02  1.76216010e-02 -3.19461711e-02 ... -2.26501226e-02\n",
            "   -3.85542847e-02  7.34607037e-03]\n",
            "  [ 1.10267606e-02  1.88875925e-02 -4.52418998e-02 ... -5.67804184e-03\n",
            "   -4.31752838e-02  2.02376395e-03]\n",
            "  ...\n",
            "  [ 1.43697811e-02  1.01407114e-02 -4.29069363e-02 ... -1.26526505e-02\n",
            "   -3.69374119e-02  1.28764901e-02]\n",
            "  [ 1.38883535e-02  2.00235564e-02 -3.69217992e-02 ... -1.73620190e-02\n",
            "   -3.04622594e-02  1.52138630e-02]\n",
            "  [ 2.32880134e-02  9.76158772e-03 -3.76149900e-02 ... -1.05245244e-02\n",
            "   -3.52670625e-02  1.04524354e-02]]\n",
            "\n",
            " [[ 4.03291844e-02  3.62152755e-02  3.23527399e-03 ...  1.31464545e-02\n",
            "   -1.86367203e-02 -1.08450847e-02]\n",
            "  [ 3.64651494e-02  2.19395552e-02  1.26066105e-02 ...  1.04880361e-02\n",
            "   -2.64335163e-02 -5.92338666e-03]\n",
            "  [ 3.36537026e-02  2.85295714e-02  4.68105217e-03 ...  1.70947667e-02\n",
            "   -1.84716284e-02 -5.36526041e-03]\n",
            "  ...\n",
            "  [ 4.26181480e-02  3.61542925e-02  8.92428681e-03 ...  1.75846592e-02\n",
            "   -1.63074713e-02 -6.79405266e-03]\n",
            "  [ 3.52732651e-02  2.96909735e-02  9.18108411e-03 ...  1.60088167e-02\n",
            "   -2.97596324e-02 -2.18837918e-03]\n",
            "  [ 3.44205499e-02  3.00818924e-02  9.66987852e-03 ...  1.34017766e-02\n",
            "   -2.56809928e-02 -3.59652302e-04]]\n",
            "\n",
            " [[ 3.97262499e-02 -1.22085977e-02  3.27079967e-02 ... -8.91118660e-04\n",
            "    1.81087367e-02 -1.58794466e-02]\n",
            "  [ 4.28043902e-02 -3.63283372e-03  3.20133194e-02 ...  7.46939113e-05\n",
            "    1.07429111e-02 -1.71490889e-02]\n",
            "  [ 4.35133651e-02 -9.38948142e-06  3.27959768e-02 ... -4.73721372e-03\n",
            "    1.22802062e-02 -1.63591653e-02]\n",
            "  ...\n",
            "  [ 4.50930372e-02 -6.76074857e-03  3.90947014e-02 ...  1.14710694e-02\n",
            "    1.87991988e-02 -1.88211296e-02]\n",
            "  [ 4.35038097e-02 -1.07171740e-02  3.50632034e-02 ...  7.28519692e-04\n",
            "    1.43610500e-02 -1.90959238e-02]\n",
            "  [ 4.24253605e-02 -1.07483929e-02  3.39910835e-02 ... -5.51419100e-04\n",
            "    1.81562491e-02 -1.13712838e-02]]], shape=(32, 200, 768), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "class MultiHeadAttention(Layer):\n",
        "  def __init__(self, heads, embedding_dim, **kwargs):\n",
        "    super(MultiHeadAttention, self).__init__(**kwargs)\n",
        "    self.heads = heads\n",
        "    self.embedding_dim = embedding_dim\n",
        "    self.self_attention = SelfAttention()\n",
        "    self.W_q = Dense(embedding_dim)\n",
        "    self.W_k = Dense(embedding_dim)\n",
        "    self.W_v = Dense(embedding_dim)\n",
        "    self.W_o = Dense(embedding_dim)\n",
        "\n",
        "  def split_heads(self, x, batch_size):\n",
        "    x = tf.reshape(x, (batch_size, -1, self.heads, self.embedding_dim // self.heads))\n",
        "    return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "\n",
        "  def concatenate_heads(self, attention, batch_size):\n",
        "    attention = tf.transpose(attention, perm=[0, 2, 1, 3])\n",
        "    return tf.reshape(attention, (batch_size, -1, self.heads * (self.embedding_dim // self.heads)))\n",
        "\n",
        "\n",
        "  def call(self, queries, keys, values, mask=None):\n",
        "\n",
        "    # Extract batch size (32)\n",
        "    batch_size = tf.shape(queries)[0]\n",
        "\n",
        "    # Split the queries, keys, values from\n",
        "    # (32, 200, 768) -> (32, 12, 200, 64)\n",
        "    Q = self.split_heads(self.W_q(queries), batch_size)\n",
        "    K = self.split_heads(self.W_k(keys), batch_size)\n",
        "    V = self.split_heads(self.W_v(values), batch_size)\n",
        "\n",
        "    # Apply attention to all 12 heads\n",
        "    attention = self.self_attention(Q, K, V, keys_dim=self.embedding_dim // self.heads, mask=mask)\n",
        "\n",
        "    # Concatenate all heads together\n",
        "    # (32, 12, 200, 64) -> (32, 200, 768)\n",
        "    concatenated_attention = self.concatenate_heads(attention, batch_size)\n",
        "\n",
        "    # Calculate a last linear transformation to get the output\n",
        "    output = self.W_o(concatenated_attention)\n",
        "\n",
        "    return output\n",
        "\n",
        "# Test\n",
        "query = tf.random.normal((BATCH_SIZE, SEQ_LENGTH, KEY_DIM))\n",
        "keys = tf.random.normal((BATCH_SIZE, SEQ_LENGTH, KEY_DIM))\n",
        "values = tf.random.normal((BATCH_SIZE, SEQ_LENGTH, KEY_DIM))\n",
        "\n",
        "multi_head_attention = MultiHeadAttention(HEADS, EMBEDDING_DIM)\n",
        "output = multi_head_attention(query, keys, values)\n",
        "print(f\"Output shape: {output.shape}\")\n",
        "print(output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q7skxQhXQ85H",
        "outputId": "7843d480-ad4c-4052-a086-d8e150ef4812"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              "       [0, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              "       [0, 0, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              "       [0, 0, 0, 1, 1, 1, 1, 1, 1, 1],\n",
              "       [0, 0, 0, 0, 1, 1, 1, 1, 1, 1],\n",
              "       [0, 0, 0, 0, 0, 1, 1, 1, 1, 1],\n",
              "       [0, 0, 0, 0, 0, 0, 1, 1, 1, 1],\n",
              "       [0, 0, 0, 0, 0, 0, 0, 1, 1, 1],\n",
              "       [0, 0, 0, 0, 0, 0, 0, 0, 1, 1],\n",
              "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]], dtype=int32)"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def causal_attention_mask(batch_size, n_dest, n_src, dtype):\n",
        "    i = tf.range(n_dest)[:, None]\n",
        "    j = tf.range(n_src)\n",
        "    m = i >= j - n_src + n_dest\n",
        "    mask = tf.cast(m, dtype)\n",
        "    mask = tf.reshape(mask, [1, n_dest, n_src])\n",
        "    mult = tf.concat(\n",
        "        [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)], 0\n",
        "    )\n",
        "    return tf.tile(mask, mult)\n",
        "np.transpose(causal_attention_mask(1, 10, 10, dtype=tf.int32)[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YPtABRBFRAJq"
      },
      "outputs": [],
      "source": [
        "class TransformerBlock(Layer):\n",
        "  def __init__(self, heads, embedding_dim, dense_1_neurons, dropout_rate, **kwargs):\n",
        "    super(TransformerBlock, self).__init__(**kwargs)\n",
        "\n",
        "    self.dropout_rate = dropout_rate\n",
        "\n",
        "    # Multi-head attention layer\n",
        "    self.multi_head_attention = MultiHeadAttention(heads, embedding_dim)\n",
        "\n",
        "    # FFN layers\n",
        "    self.dense_1 = Dense(dense_1_neurons, activation='relu')\n",
        "    self.dense_2 = Dense(embedding_dim)\n",
        "\n",
        "    # Normalization layers\n",
        "    self.layer_norm_1 = LayerNormalization(epsilon=1e-6)\n",
        "    self.layer_norm_2 = LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "    # Dropout layers\n",
        "    self.dropout_1 = Dropout(self.dropout_rate)\n",
        "    self.dropout_2 = Dropout(self.dropout_rate)\n",
        "\n",
        "  def causal_attention_mask(self, batch_size, n_dest, n_src, dtype):\n",
        "    i = range(n_dest)[:, None]\n",
        "    j = range(n_src)\n",
        "    m = i >= j - n_src + n_dest\n",
        "    mask = cast(m, dtype)\n",
        "    mask = reshape(mask, [1, n_dest, n_src])\n",
        "    mult = concat(\n",
        "        [expand_dims(batch_size, -1), constant([1, 1], dtype=int32)], 0\n",
        "    )\n",
        "    return tile(mask, mult)\n",
        "\n",
        "\n",
        "  def call(self, x):\n",
        "\n",
        "    input_shape = shape(x)\n",
        "    batch_size = input_shape[0]\n",
        "    seq_len = input_shape[1]\n",
        "    causal_mask = self.causal_attention_mask(\n",
        "        batch_size, seq_len, seq_len, bool\n",
        "    )\n",
        "\n",
        "    # Multi-Head Attention\n",
        "    attention = self.multi_head_attention(x, x, x, mask=causal_mask)\n",
        "\n",
        "    # Dropout\n",
        "    out_drop_1 = self.dropout_1(attention)\n",
        "\n",
        "    # Residual connection + Layer normalization\n",
        "    res_1 = x + out_drop_1\n",
        "    out_ln_1 = self.layer_norm_1(res_1)\n",
        "\n",
        "    # FFN layers\n",
        "    out_dense_1 = self.dense_1(out_ln_1)\n",
        "    out_dense_2 = self.dense_2(out_dense_1)\n",
        "\n",
        "    # Dropout\n",
        "    out_drop_2 = self.dropout_2(out_dense_2)\n",
        "\n",
        "    # Residual connection + Layer normalization\n",
        "    res_2 = out_ln_1 + out_drop_2\n",
        "    output = self.layer_norm_2(res_2)\n",
        "\n",
        "    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s79JdTCXpiZF"
      },
      "outputs": [],
      "source": [
        "# Input layer\n",
        "inputs = Input(shape=(None,), dtype=tf.int32)\n",
        "\n",
        "# Positional Embedding\n",
        "out_pos = PositionalEmbedding(\n",
        "            vocab_size=VOCAB_SIZE,\n",
        "            embedding_dim=EMBEDDING_DIM,\n",
        "            max_unique_pos_embed=MAX_LEN_UNIQUE_POS_EMBED\n",
        "        )(inputs)\n",
        "\n",
        "# Transformer block\n",
        "out_tf = TransformerBlock(\n",
        "            heads=HEADS,\n",
        "            embedding_dim=EMBEDDING_DIM,\n",
        "            dense_1_neurons=DENSE_1_NEURONS,\n",
        "            dropout_rate=DROP_RATE\n",
        "        )(out_pos)\n",
        "\n",
        "# Output layer\n",
        "outputs = Dense(VOCAB_SIZE, activation=\"softmax\")(out_tf)\n",
        "\n",
        "transformer_model = Model(inputs=inputs, outputs=outputs)\n",
        "transformer_model.compile(\"adam\", loss=[tf.keras.losses.SparseCategoricalCrossentropy(), None])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LwxExm2s6oK_"
      },
      "source": [
        "## 4. Train model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XwMKxE6mQVel"
      },
      "outputs": [],
      "source": [
        "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=\"./checkpoint/checkpoint.weights.h5\",\n",
        "    save_weights_only=True,\n",
        "    save_freq=\"epoch\",\n",
        "    verbose=0,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "id": "bqDdsC59fQTN",
        "outputId": "b0ae4c1f-1093-4977-8347-2aff24f1b29d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4928s\u001b[0m 157ms/step - loss: 0.6567\n",
            "Epoch 2/5\n",
            "\u001b[1m31250/31250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4859s\u001b[0m 155ms/step - loss: 0.5870\n",
            "Epoch 3/5\n",
            "\u001b[1m  672/31250\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:18:42\u001b[0m 154ms/step - loss: 0.5970"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-2caa4733c561>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m hist = transformer_model.fit(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel_checkpoint_callback\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    369\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mepoch_iterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 371\u001b[0;31m                     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    372\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/backend/tensorflow/trainer.py\u001b[0m in \u001b[0;36mfunction\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    218\u001b[0m             ):\n\u001b[1;32m    219\u001b[0m                 \u001b[0mopt_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmulti_step_on_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mopt_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mopt_outputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/data/ops/optional_ops.py\u001b[0m in \u001b[0;36mhas_value\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    174\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mhas_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolocate_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m       return gen_optional_ops.optional_has_value(\n\u001b[0m\u001b[1;32m    177\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/ops/gen_optional_ops.py\u001b[0m in \u001b[0;36moptional_has_value\u001b[0;34m(optional, name)\u001b[0m\n\u001b[1;32m    170\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[1;32m    173\u001b[0m         _ctx, \"OptionalHasValue\", name, optional)\n\u001b[1;32m    174\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "hist = transformer_model.fit(\n",
        "    train_dataset,\n",
        "    epochs=EPOCHS,\n",
        "    callbacks=[model_checkpoint_callback]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qq8CH4_QRuLR"
      },
      "outputs": [],
      "source": [
        "transformer_model.save(MODEL_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LOecDSOPRe1V"
      },
      "source": [
        "## 5. Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7pGJqnNyRizf"
      },
      "outputs": [],
      "source": [
        "class ReviewGenerator():\n",
        "  def __init__(self, model, vocab):\n",
        "    self.vocab = vocab\n",
        "    self.model = model\n",
        "    self.word_to_index = {\n",
        "            word: index for index, word in enumerate(vocab)\n",
        "        }\n",
        "\n",
        "  def get_next_token(self, probs, temperature):\n",
        "        probs = probs ** (1 / temperature)\n",
        "        probs = probs / np.sum(probs)\n",
        "        return np.random.choice(len(probs), p=probs), probs\n",
        "\n",
        "  def generate(self, start_prompt, max_tokens, temperature):\n",
        "\n",
        "    start_tokens = [\n",
        "            self.word_to_index.get(x, 1) for x in start_prompt.split()\n",
        "        ]\n",
        "\n",
        "    next_predicted_token = None\n",
        "    generated_text = \"\"\n",
        "    while len(start_tokens) < max_tokens and next_predicted_token != 0:\n",
        "\n",
        "      x = np.array([start_tokens])\n",
        "      y_pred, _ = self.model.predict(x, verbose=0)\n",
        "\n",
        "      next_predicted_token, probs = self.get_next_token(y_pred[0][-1], temperature)\n",
        "      start_tokens.append(next_predicted_token)\n",
        "\n",
        "      generated_text += self.vocab[next_predicted_token] + \" \"\n",
        "\n",
        "    return generated_text\n",
        "\n",
        "review_generator = ReviewGenerator(transformer_model, vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rQVi28IBWMXg",
        "outputId": "0b894c2c-41bf-455e-d5f4-a31992e1a604"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Promp: movie review for Adrift |\n",
            "Generated review: it ' s fitting that the actors lead to [UNK] the film with flashy acting highlights , yet doesn ' t seem far too subtle , beneath the surface , and no human [UNK] .  \n"
          ]
        }
      ],
      "source": [
        "prompt = \"movie review for Adrift |\"\n",
        "generated_text = review_generator.generate(prompt, max_tokens=70, temperature=0.9)\n",
        "\n",
        "print(f\"Promp: {prompt}\")\n",
        "print(f\"Generated review: {generated_text}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "uDHbxMLg6dux",
        "LOecDSOPRe1V"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
